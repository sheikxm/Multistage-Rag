# # -*- coding: utf-8 -*-
# """multistagerag.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1yN13UNhs1GS-b7pq4VHROgJeiUlqnqKG
# """

# !pip  -q install datasets sentence-transformers faiss-cpu langchain

# !pip install datasets sentence-transformers faiss-cpu transformers

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

dataset = load_dataset("hotpotqa/hotpot_qa", "distractor") # or "fullwiki"

print("Sample data: ", dataset['train'][0])



from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

doc = dataset['train'][0]['context']

print(doc)

docs = []
for title, sentences in zip(doc['title'], doc['sentences']):
    full_text = " ".join(sentences)
    split_docs = text_splitter.split_text(full_text)
    docs.extend(split_docs)

print("Number of chunks created: ", len(docs))
print("First chunk: ", docs[0])

# !pip install langchain-community

from langchain.embeddings import SentenceTransformerEmbeddings
embeddings =SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

embeddingss = embeddings.embed_documents(docs)
embedding_array = np.array(embeddingss)
print("Embedding shape: ", embedding_array[0].shape)

from langchain.vectorstores import FAISS
vectorstore = FAISS.from_texts(docs, embeddings)

vectorstore.add_texts(docs)

retriever = vectorstore.as_retriever()

query = "Which magazine was started first Arthur's Magazine or First for Women?"
retrieved_docs = retriever.get_relevant_documents(query, k=5)

print("Top 5 retrieved passages for query: ", query)
for i, doc in enumerate(retrieved_docs):
    print(f"Passage {i+1}: {doc}")

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

def rerank_passages(query, passages, reranker_model, reranker_tokenizer):
    # Prepare query and passage inputs
    # Assuming 'Document' objects have a 'page_content' attribute containing the passage text
    # If your Document objects have a different attribute for the text,
    # replace 'page_content' with the correct attribute name
    inputs = [query + " [SEP] " + passage.page_content for passage in passages]
    tokenized_inputs = reranker_tokenizer(inputs, padding=True, truncation=True, return_tensors="pt")

    # Perform inference to get relevance scores
    with torch.no_grad():
        outputs = reranker_model(**tokenized_inputs)
        scores = outputs.logits.squeeze(-1).tolist()  # Get the logits for each passage

    # Sort passages based on the scores in descending order
    # Use the original 'passage' object in the ranking, not just the text
    ranked_passages = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)

    # Return the ranked passages along with their scores
    return [(passage, score) for passage, score in ranked_passages]

reranker_model, reranker_tokenizer = load_reranker('cross-encoder/ms-marco-MiniLM-L-12-v2')

reranked_passages = rerank_passages(query, retrieved_docs, reranker_model, reranker_tokenizer)


for i, (passage, score) in enumerate(reranked_passages):
    print(f"Rank {i+1}: {passage} (Score: {score})")

